{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The New York Social Graph\n",
    "\n",
    "[New York Social Diary](http://www.newyorksocialdiary.com/) provides a\n",
    "fascinating lens onto New York's socially well-to-do.  The data forms a natural social graph for New York's social elite.  Take a look at this page of a recent [run-of-the-mill holiday party](http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers).\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "For this project, we will assemble the social graph from photo captions for parties dated December 1, 2014, and before.  Using this graph, we can make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "We will attack the project in three phases:\n",
    "1. Get a list of all the photo pages to be analyzed.\n",
    "2. Parse all of the captions on a sample page.\n",
    "3. Parse all of the captions on all pages, and assemble the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase One\n",
    "\n",
    "The first step is to crawl the data.  We want photos from parties on or before December 1st, 2014.  Go to the [Party Pictures Archive](http://www.newyorksocialdiary.com/party-pictures) to see a list of (party) pages.  We want to get the url for each party page, along with its date.\n",
    "\n",
    "Here are some packagest that you may find useful.  You are welcome to use others, if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We recommend using Python [Requests](http://docs.python-requests.org/en/master/) to download the HTML pages, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to process the HTML.  Let's start by getting the [first page](http://www.newyorksocialdiary.com/party-pictures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"http://www.newyorksocialdiary.com/party-pictures\"\n",
    "page = requests.get(url, params={\"limit\": 1000, \"offset\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we process the text of the page with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This page has links to 50 party pages. Look at the structure of the page and determine how to isolate those links.  Your browser's developer tools (usually `Cmd`-`Option`-`I` on Mac, `Ctrl`-`Shift`-`I` on others) offer helpful tools to explore the structure of the HTML page.\n",
    "\n",
    "Once you have found a patter, use BeautifulSoup's [select](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors) or [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) methods to get those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "links = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There should be 50 per page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a look at that first link.  Figure out how to extract the URL of the link, as well as the date.  You probably want to use `datetime.strptime`.  See the [format codes for dates](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "link = links[0]\n",
    "# Check that the title and date match what you see visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For purposes of code reuse, let's put that logic into a function.  It should take the link element and return the URL and date parsed from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_link_date(element):\n",
    "    link_tuple=[]\n",
    "    for link in element:\n",
    "        link_url = link.select('a')[0]['href']\n",
    "        link_date = link.select('span')[3].text\n",
    "        link_tuple.append((link_url, link_date))\n",
    "    return link_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You may want to check that it works as you expected.\n",
    "\n",
    "Once that's working, let's write another function to parse all of the links on a page.  Thinking ahead, we can make it take a Requests [Response](http://docs.python-requests.org/en/master/api/#requests.Response) object and do the BeautifulSoup parsing within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_links(response):\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    div = soup.find_all('div', attrs={'class': 'views-row'}) \n",
    "    return get_link_date(div) # A list of URL, date pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we run this on the previous response, we should get 50 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert len(get_links(page)) == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But we only want parties with dates on or before the first of December, 2014.  Let's write a function to filter our list of dates to those at or before a cutoff.  Using a keyword argument, we can put in a default cutoff, but allow us to test with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the default cutoff, there should be no valid parties on the first page.  Adjust the cutoff date to check that it is actually working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def filter_by_date(links, cutoff=datetime(2014, 12, 1)):\n",
    "    # Return only the elements with date <= cutoff\n",
    "    valid_list=[]\n",
    "    for l,d in links:\n",
    "        date = datetime.strptime(d,'%A, %B %d, %Y')\n",
    "        if (date <= cutoff):\n",
    "            valid_list.append((l,date))\n",
    "    return valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert len(filter_by_date(get_links(page))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we should be ready to get all of the party URLs.  Click through a few of the index pages to determine how the URL changes.  Figure out a strategy to visit all of them.\n",
    "\n",
    "HTTP requests are generally IO-bound.  This means that most of the time is spent waiting for the remote server to respond.  If you use `requests` directly, you can only wait on one response at a time.  [requests-futures](https://github.com/ross/requests-futures) lets you wait for multiple requests at a time.  You may wish to use this to speed up the downloading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "link_list = []\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "urls = [\"http://www.newyorksocialdiary.com/party-pictures?page=\" + str(i) for i in range(29)]\n",
    "futures = [session.get(url) for url in urls]    \n",
    "\n",
    "for future in futures:\n",
    "    link_list.extend(filter_by_date(get_links(future.result())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the end, you should have 1193 parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(link_list) == 1193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/party-pictures/2014/gala-guests', datetime.datetime(2014, 11, 24, 0, 0))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In case we need to restart the notebook, we should save this information to a file.  There are many ways you could do this; here's one using `dill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dill.dump(link_list, open('nysd-links.pkd', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To restore the list, we can just load it from the file.  When the notebook is restarted, you can skip the code above and just run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "link_list = dill.load(open('nysd-links.pkd', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 1: histogram\n",
    "\n",
    "Get the number of party pages for the 95 months (that is, month-year pair) in the data.  Notice that while the party codes might be written as \"FRIDAY, FEBRUARY 28, 2014\", in this output, you would have to represent the month-year code as \"Feb-2014\".  This can all be done with `strfime` and the [format codes for dates](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior).\n",
    "\n",
    "Plot the histogram for yourself.  Do you see any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def histogram():\n",
    "    dic={}\n",
    "    for l,d in link_list:\n",
    "        monthyear = d.strftime('%b-%Y')\n",
    "        if monthyear in dic:\n",
    "            dic[monthyear] += 1\n",
    "        else:\n",
    "            dic[monthyear] = 1\n",
    "        \n",
    "    return dic.items()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#def histogram():\n",
    "#    return [(\"Dec-2014\", 1)] * 95  # Replace with the correct list\n",
    "\n",
    "#grader.score(question_name='graph__histogram', func=histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase Two\n",
    "\n",
    "In this phase, we we concentrate on getting the names out of captions for a given page.  We'll start with [the benefit cocktails and dinner](http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood) for [Lenox Hill Neighborhood House](http://www.lenoxhill.org/), a neighborhood organization for the East Side.\n",
    "\n",
    "Take a look at that page.  Note that some of the text on the page is captions, but others are descriptions of the event.  Determine how to select only the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "captions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "By our count, there are about 110.  But if you're off by a couple, you're probably okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert abs(len(captions) - 110) < 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's encapsulate this in a function.  As with the links pages, we want to avoid downloading a given page the next time we need to run the notebook.  While we could save the files by hand, as we did before, a checkpointing library like [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) can handle this for you.  (Note, though, that you may not want to enable this until you are sure that your function is working.)\n",
    "\n",
    "You should also keep in mind that HTTP requests fail occasionally, for transient reasons.  You should plan how to detect and react to these failures.   The [retrying module](https://pypi.python.org/pypi/retrying) is one way to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Glenn Adamson, Simon Doonan, Victoire de Castellane, Craig Leavitt, Jerome Chazen, Andi Potamkin, Ralph Pucci, Kirsten Bailey, Edwin Hathaway, and Dennis Freedman at the Museum of Art and Design's annual MAD BALL. \",\n",
       " u' Randy Takian ',\n",
       " u' Kamie Lightburn and Christopher Spitzmiller ',\n",
       " u' Christopher Spitzmiller and Diana Quasha ',\n",
       " u' Mariam Azarm, Sana Sabbagh, and Lynette Dallas ',\n",
       " u' Christopher Spitzmiller, Sydney Shuman, and Matthew Bees',\n",
       " u' Christopher Spitzmiller and Tom Edelman ',\n",
       " u' Warren Scharf and Sydney Shuman ',\n",
       " u' Amory McAndrew and Sean McAndrew ',\n",
       " u' Sydney Shuman, Mario Buatta, and Helene Tilney ',\n",
       " u' Katherine DeConti and Elijah Duckworth-Schachter ',\n",
       " u' John Rosselli and Elizabeth Swartz ',\n",
       " u' Stephen Simcock, Lee Strock, and Thomas Hammer ',\n",
       " u' Searcy Dryden, Lesley Dryden, Richard Lightburn, and Michel Witmer ',\n",
       " u' Jennifer Cacioppo and Kevin Michael Barba ',\n",
       " u' Virginia Wilbanks and Lacary Sharpe ',\n",
       " u' Valentin Hernandez, Yaz Hernandez, Chele Farley, and James Farley',\n",
       " u' Harry Heissmann, Angela Clofine, and Michael Clofine',\n",
       " u' Jared Goss and Kristina Stewart Ward ',\n",
       " u' Alex Papachristidis and Mario Buatta ',\n",
       " u' Nick Olsen, Lindsey Coral Harper, Alberto Villalobos, and David Duncan ',\n",
       " u' Caroline Dean, Christopher Spitzmiller, and Ellen Niven ',\n",
       " u' Debbie Bancroft and David Svanda ',\n",
       " u' Julia Weld and Christopher Spitzmiller ',\n",
       " u' Kevin Lichten, Robert Ruffino, Joan Craig, and Michael McGraw ',\n",
       " u' Roric Tobin, Chele Farley, Richard Farley, and Geoffrey Bradfield',\n",
       " u' Lacary Sharpe, Kamie Lightburn, and Emily Leonard ',\n",
       " u' Dr. Doug Steinbrech and Mary Van Pelt ',\n",
       " u' Kathy  and Othon Prounis ',\n",
       " u'The MAD dinner scene',\n",
       " u'Victoire de Castellane, Dennis Freedman, Andi Potamkin, and Ralph Pucci',\n",
       " u'Donald Tober, Barbara Tober, Linda Fargo, and Bjorn Wallander',\n",
       " u'Andi Potamkin, Glenn Adamson, Ralph Pucci, Victoire de Castellane, Dennis Freedman, and Craig Leavitt',\n",
       " u'Celia Morrissette and Keith Johnson',\n",
       " u'Simon Doonan',\n",
       " u'Sandra Gering and Julia Kuni',\n",
       " u'Alexandra Richards and Paul Longo',\n",
       " u'Bryna and Martin Pomp',\n",
       " u'Neville Wakefield',\n",
       " u'Caroline Cokley',\n",
       " u'Ryan Davis',\n",
       " u'Freddie Leiba and Cynthia Adler',\n",
       " u'Rick and Leticia Presutti',\n",
       " u' Glenn Adamson and Ebony G. Patterson',\n",
       " u'Lowery Stokes Sims and C. Virginia Fields',\n",
       " u'Barbara Regna',\n",
       " u'Deborah Lloyd',\n",
       " u'Kelly  and Jay Sugarman',\n",
       " u'Steve Evans, Kate White, Liz Nacey, Anne Strickland, Patrick Jones, and Jennifer Fujitani',\n",
       " u'Wendy Diamond',\n",
       " u'Tood Eberle and Gina Nanni',\n",
       " u'Linda Fargo and Bjorn Wallander',\n",
       " u'Tony Ingrao and Randy Kemper',\n",
       " u'C. Virginia Fields, Gordan Kenney, and Jill Huggins',\n",
       " u'Wendy MacGaw and Howard Ben Tre',\n",
       " u'Laura  and Lewis Kruger',\n",
       " u' The Waldorf Astoria Ballroom ',\n",
       " u' Deborah Marton (Executive Director of the NYRP) ',\n",
       " u' Bette Midler ',\n",
       " u' Lance LePere and Michael Kors ',\n",
       " u'A table centerpiece',\n",
       " u\" Linda Allard's party \",\n",
       " u' Mrs. and Mrs. Ben Needell Esq. ',\n",
       " u' Nile Rodgers and Bette Midler ',\n",
       " u' Todd DeGarmo and Deborah Marton ',\n",
       " u' Samuel Kelly ',\n",
       " u' David and Lola Rockwell ',\n",
       " u' Davon Windsor and Rachel Hilbert ',\n",
       " u'Hula girls welcome the guests',\n",
       " u' Honorees Sheryl and and Dan Tishman with Bette Midler',\n",
       " u' Darcy Stacom ',\n",
       " u' Judy Gold and Bette Midler ',\n",
       " u' Kevin and Michelle Fox',\n",
       " u' Jane Krakowski, Michael Kors, Bette Midler, and Lance LePere ',\n",
       " u'\\n\\n\\n Shoshanna Gruss and Dr. Drew Schiff \\n\\n\\n',\n",
       " u'\\n\\n\\n Margo and Jimmy Nederlander\\n\\n\\n\\n',\n",
       " u'\\n\\n\\n Nederlander table as Zombie Gillian\\u2019s Island \\n\\n\\n',\n",
       " u'\\n\\n\\n Jon Recor\\n\\n\\n\\n',\n",
       " u'\\n\\n\\n Shelly Malkin and Nathalie Gerschel Kaplan \\n\\n\\n',\n",
       " u' Zombie dancers in silver corridor ',\n",
       " u' Douglas Little and friend',\n",
       " u' Zombie airplane staff directing guests around the party ',\n",
       " u' Martin von Haselberg ',\n",
       " u' Mr. and Mrs. Allen Swerdlick ',\n",
       " u\" Ta'Rhonda Jones of Empire (on right) \",\n",
       " u' Lance LePere, Jane Krakowski, and Michael Kors ',\n",
       " u' Kelly Bensimon and John Demsey',\n",
       " u' Michael Kors and Lauren duPont ',\n",
       " u' Nancy Hunt, Bette Midler, and Nile Rodgers ',\n",
       " u' Nile Rodgers & CHIC with Bette Midler on stage ',\n",
       " u' Thriller zombie dancers ',\n",
       " u'Richard and Donna Soloway',\n",
       " u'Barbara  and Peter Regna',\n",
       " u'Jonas Barcellos and friends',\n",
       " u'Michelle Halpern, Mark Robinson, and Lisa Halpern',\n",
       " u'Clark  and Esra Munnell',\n",
       " u'Elizabeth Johnson and friends',\n",
       " u'Wendy Carduner and George Perry',\n",
       " u'Lisa and Tom Wilkenson',\n",
       " u' Stacy Reilly and friends',\n",
       " u'Eveyln Subramaniam',\n",
       " u'Sarah and Chips Page',\n",
       " u'Mrs. DeMaurier and friend',\n",
       " u'Jim Killerlane and Allison Minton',\n",
       " u'Liana Makkos',\n",
       " u'Ken and Maria Fishel, Tina Wong, and George Perry',\n",
       " u'Marisa Rose and \"KISS\" friends',\n",
       " u'Eveyln Subramaniam',\n",
       " u'Alexis Mersentes and Doris Liebman',\n",
       " u'\\n        Photographs by Mia McDonald, Anna Yatskevich, & Getty Images (NYRP); Annie Watt (Lenox Hill); MAD (BFA); Cutty McGill (Doubles)']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "div = soup.find_all(['div', 'td'], attrs={'class': 'photocaption'}) \n",
    "captions = []\n",
    "for e in div:\n",
    "    captions.append(e.text)\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert abs(len(captions) - 110) < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_names(captions):\n",
    "    import re\n",
    "    names_list = []\n",
    "    for caps in captions:\n",
    "        expression = re.compile('[A-Z]\\w+\\s[A-Z]\\w+')\n",
    "        names = re.findall(expression,caps)\n",
    "        for n in names:\n",
    "            names_list.append(n)\n",
    "        \n",
    "    return names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#caps = caps.strip()\n",
    "#if ';' in caps or len(caps)<3:\n",
    "#    continue\n",
    "#    print caps\n",
    "#caps = caps.replace('and',',')\n",
    "#caps = caps.replace('\\n','')\n",
    "#caps = caps.replace('\\r','')\n",
    "#caps = caps.split(',')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "name_stub = '[A-Z][a-z\\']{0,2}[A-Z]?[a-z.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = get_captions(captions)\n",
    "len(names)\n",
    "#names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shelly BOB', 'Nathalie Gerschel']"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ' Shelly Malkin and Nathalie Gerschel Kaplan \\n\\n'\n",
    "expression = re.compile('[A-Z]\\w+\\s[A-Z]\\w+')\n",
    "re.findall(expression,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert captions == get_captions(\"/party-pictures/2015/celebrating-the-neighborhood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have some sample captions, let's start parsing names out of those captions.  There are many ways of going about this, and we leave the details up to you.  Some issues to consider:\n",
    "\n",
    "  1. Some captions are not useful: they contain long narrative texts that explain the event.  Try to find some heuristic rules to separate captions that are a list of names from those that are not.  A few heuristics include:\n",
    "    - look for sentences (which have verbs) and as opposed to lists of nouns. For example, [nltk does part of speech tagging](http://www.nltk.org/book/ch05.html) but it is a little slow. There may also be heuristics that accomplish the same thing.\n",
    "    - Similarly, spaCy's [entity recognition](https://spacy.io/docs/usage/entity-recognition) couble be useful here.\n",
    "    - Look for commonly repeated threads (e.g. you might end up picking up the photo credits or people such as \"a friend\").\n",
    "    - Long captions are often not lists of people.  The cutoff is subjective, but for grading purposes, *set that cutoff at 250 characters*.\n",
    "  2. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`. **Note**: The reference solution uses regex exclusively for name parsing.\n",
    "  3. You might find a person named \"ra Lebenthal\".  There is no one by this name.  Can anyone spot what's happening here?\n",
    "  4. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other ('optional') titles that are being used?  They should probably be filtered out because they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "  5. There is a special case you might find where couples are written as eg. \"John and Mary Smith\". You will need to write some extra logic to make sure this properly parses to two names: \"John Smith\" and \"Mary Smith\".\n",
    "  6. When parsing names from captions, it can help to look at your output frequently and address the problems that you see coming up, iterating until you have a list that looks reasonable. This is the approach used in the reference solution. Because we can only asymptotically approach perfect identification and entity matching, we have to stop somewhere.\n",
    "  \n",
    "**Questions worth considering:**\n",
    "  1. Who is Patrick McMullan and should he be included in the results? How would you address this?\n",
    "  2. What else could you do to improve the quality of the graph's information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_names():\n",
    "    new_captions = get_captions(captions)\n",
    "    sorted_captions = sorted(list(set(new_captions)))\n",
    "    return sorted_captions[:100]\n",
    "    #return [\"Caroline Dean\"] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sample_names()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2: sample_names\n",
    "\n",
    "Once you feel that your algorithm is working well on these captions, parse all of the captions and extract all the names mentioned.  Sort them alphabetically, by first name, and return the first hundred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.9\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#def sample_names():\n",
    "#    return [\"Caroline Dean\"] * 100\n",
    "\n",
    "#grader.score(question_name='graph__sample_names', func=sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, run this sort of test on a few other pages.  You will probably find that other pages have a slightly different HTML structure, as well as new captions that trip up your caption parser.  But don't worry if the parser isn't perfect -- just try to get the easy cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase Three\n",
    "\n",
    "Once you are satisfied that your caption scraper and parser are working, run this for all of the pages.  If you haven't implemented some caching of the captions, you probably want to do this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = \"http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# soup = BeautifulSoup(response.text, \"lxml\")\n",
    "# div = soup.find_all(['div', 'td'], attrs={'class': 'photocaption'}) \n",
    "# captions = []\n",
    "# for e in div:\n",
    "#     captions.append(e.text)\n",
    "# captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Scraping all of the pages could take 10 minutes or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_captions_all(response):\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    div = soup.find_all(['div', 'tr', 'td'], attrs={'class': 'photocaption'}) \n",
    "    captions = []\n",
    "    for e in div:\n",
    "        captions.append(e.text)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions = []\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "urls = [\"http://www.newyorksocialdiary.com/\"+str(l[0]) for l in link_list] \n",
    "#urls = [\"http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood\"]\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "futures = [session.get(url) for url in urls]    \n",
    "\n",
    "for future in futures:\n",
    "    all_captions.extend(get_captions_all(future.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125701"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_names = []\n",
    "all_names = get_names(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224162"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_names)\n",
    "#len(set(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the remaining analysis, we think of the problem in terms of a\n",
    "[network](http://en.wikipedia.org/wiki/Computer_network) or a\n",
    "[graph](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29).  Any time a pair of people appear in a photo together, that is considered a link.  What we have described is more appropriately called an (undirected)\n",
    "[multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops but this has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).  In this problem, we will analyze the social graph of the new york social elite.  We recommend using python's [networkx](https://networkx.github.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted_names = sorted(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# junk = ['Junior','Benefit','Client','Clinical','Associate','President','Director','Board','Executive','Studio','AA','News','Dancers','II','Dancers','Lifetime','Trustee','Music','friend']\n",
    "\n",
    "# all_name_new = [n for n in sorted_names\n",
    "#        if not any(word in n for word in junk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sorted(all_name_new)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All in all, you should end up with over 100,000 captions and more than 110,000 names, connected in about 200,000 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def degree_counter():\n",
    "    degree_list = []\n",
    "    for name,count in Counter(all_name_new).most_common(100):\n",
    "        degree_list.append((name.encode(\"utf-8\"),count))\n",
    "    return degree_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 3: degree\n",
    "\n",
    "The simplest question to ask is \"who is the most popular\"?  The easiest way to answer this question is to look at how many connections everyone has.  Return the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 189.92\n",
    "    \"std\": 87.8053034454\n",
    "    \"min\": 124.0\n",
    "    \"25%\": 138.0\n",
    "    \"50%\": 157.0\n",
    "    \"75%\": 195.0\n",
    "    \"max\": 666.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def name_tuple_creator(names_list):\n",
    "    import itertools\n",
    "\n",
    "    comb = []\n",
    "    for L in range(0, len(names_list)+1):\n",
    "        for subset in itertools.combinations(names_list, 2):\n",
    "            if subset not in comb:\n",
    "                #print(subset)\n",
    "                comb.append(subset)\n",
    "                \n",
    "    return comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_together_names(captions):\n",
    "    import re\n",
    "    names_list = []\n",
    "    #print captions\n",
    "    for caps in captions:\n",
    "        caps = caps.strip()\n",
    "        caps = caps.replace('\\n','')\n",
    "        expression = re.compile('[A-Z]\\w+\\s[A-Z]\\w+')\n",
    "        names = re.findall(expression,caps)\n",
    "        #new_names = []\n",
    "        #for n in names:\n",
    "        #    if len(n) > 2 and len(n) < 250: \n",
    "        #        print names\n",
    "        #        new_names.append(names)\n",
    "        if len(names) > 2 and len(names) < 250:\n",
    "            print names\n",
    "            names_list.append(name_tuple_creator(names))\n",
    "        \n",
    "    return names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_caption_names(captions):\n",
    "    import re\n",
    "    captions_names=[]\n",
    "    \n",
    "    for c in captions:\n",
    "#         if len(re.findall('[a-zA-Z]', c))>0: \n",
    "#             print c\n",
    "        if len(c) > 3:\n",
    "            names = get_names(c)\n",
    "            if len(names) > 0:\n",
    "                captions_names.append(name_tuple_creator(names))\n",
    "        \n",
    "    return captions_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_names(caption):\n",
    "    import re\n",
    "    str1 = caption.strip()\n",
    "    str1 = str1.encode(\"utf-8\")\n",
    "    #str1 = str1.replace('\\n',' ')\n",
    "    str1 = str1.replace('with',',')\n",
    "    str1 = str1.replace('and and','and')\n",
    "    list1 = re.split(\",|Mr.|Dr.|Miss|Mrs.|Jr.|M.D.|Ph.D.|PhD|CEO|Mayor|President|Honorees|Honoree|\\n\", str1)\n",
    "    list2 = [e.strip() for e in list1 if len(e.strip())>2 and e.strip() != 'and']\n",
    "    \n",
    "    names_list=[]\n",
    "    for e in list2:\n",
    "        if 'and ' in e:\n",
    "            l3= e.split('and')\n",
    "            if (len(l3[0]) == 0 and len(l3[1]) == 0):\n",
    "                continue\n",
    "            #print \"l3\", l3\n",
    "            friend_check = [True for f in l3 if \"friend\" in str(f)]\n",
    "            #print friend_check\n",
    "            if friend_check:\n",
    "                #print \"friend found: \"\n",
    "                #names_list.append(e.split('friend',1)[0])\n",
    "                e = e.split('friend',1)[0]\n",
    "                #print e\n",
    "                continue\n",
    "            if  len(l3) > 2:\n",
    "                continue\n",
    "            first = e.split('and ')[0].strip()\n",
    "            second = e.split('and ')[1].strip()\n",
    "#             print first\n",
    "#             print second\n",
    "            if len(first) == 0 and len(second) == 0:\n",
    "                continue\n",
    "            elif len(first) == 0 and len(second) != 0:\n",
    "                if second[0].isupper(): names_list.append(second)\n",
    "                #print second + \"second\"\n",
    "            elif len(second) == 0 and len(first) != 0:\n",
    "                if first[0].isupper(): names_list.append(first)\n",
    "                #print first + \"first\"\n",
    "\n",
    "    #       #for husband and wife\n",
    "            elif len(re.findall(' ',first)) == 0:\n",
    "                if (len(second.split())>1):\n",
    "                    first = first + ' '+ second.split()[1]  \n",
    "                if first[0].isupper() and second[0].isupper():\n",
    "                    names_list.append(first)\n",
    "                    names_list.append(second)\n",
    "    #                 print \"here\"\n",
    "#                 print first\n",
    "#                 print second\n",
    "            else:\n",
    "                if first[0].isupper() and second[0].isupper():\n",
    "                    names_list.append(first)\n",
    "                    names_list.append(second)\n",
    "    #     #not a person name\n",
    "        elif len(re.findall(' ',e))>2:\n",
    "            continue\n",
    "        else:\n",
    "            #print e\n",
    "            names_list.append(e.strip())\n",
    "    return names_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Mariam Azarm', 'Sana Sabbagh')],\n",
       " [('Sana Sabbagh', 'Lynette Dallas')],\n",
       " [('Kamie Lightburn', 'Christopher Spitzmiller')]]"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_sample = ['Mariam Azarm, Sana Sabbagh\\n  ','   u',' his wife and Mariam Azarm\\n\\n, Sana Sabbagh,and Lynette Dallas ',' Kamie Lightburn and Christopher Spitzmiller ']\n",
    "#cap_sample = ['Chuck Grodin \\nDiana Rosario, Ali Sussman, Sarah Boll, Jen Zaleski, Alysse Brennan, and Lindsay Macbeth and friends']\n",
    "#cap_sample = 'and '\n",
    "#cap_sample =  [' Dr. Amy Cunningham-Bussel, Ray Mirra, and Dr. Tyler Janovitz']\n",
    "#cap_sample =  [' Melissa Errico, Todd Hollander, and Natalia Bulgari '] \n",
    "#cap_sample = [' Emdens and Falkenbergs and']\n",
    "relation_names = get_caption_names(cap_sample)\n",
    "relation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_relationship[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94410"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_relationship = get_caption_names(all_captions)\n",
    "len(all_relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# all_relationship = get_together_names(all_captions)\n",
    "# len(all_relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_relationship[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207505"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list = [item for sublist in all_relationship for item in sublist]\n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Les Lieberman', 'Barri Lieberman'),\n",
       " ('Les Lieberman', 'Isabel Kallman'),\n",
       " ('Les Lieberman', 'Trish Iervolino'),\n",
       " ('Les Lieberman', 'Ron Iervolino'),\n",
       " ('Barri Lieberman', 'Isabel Kallman'),\n",
       " ('Barri Lieberman', 'Trish Iervolino'),\n",
       " ('Barri Lieberman', 'Ron Iervolino'),\n",
       " ('Isabel Kallman', 'Trish Iervolino'),\n",
       " ('Isabel Kallman', 'Ron Iervolino'),\n",
       " ('Trish Iervolino', 'Ron Iervolino')]"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Les Lieberman', 'Barri Lieberman'),\n",
       "  ('Les Lieberman', 'Isabel Kallman'),\n",
       "  ('Les Lieberman', 'Trish Iervolino'),\n",
       "  ('Les Lieberman', 'Ron Iervolino'),\n",
       "  ('Barri Lieberman', 'Isabel Kallman'),\n",
       "  ('Barri Lieberman', 'Trish Iervolino'),\n",
       "  ('Barri Lieberman', 'Ron Iervolino'),\n",
       "  ('Isabel Kallman', 'Trish Iervolino'),\n",
       "  ('Isabel Kallman', 'Ron Iervolino'),\n",
       "  ('Trish Iervolino', 'Ron Iervolino')]]"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_relationship[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def degree():\n",
    "\n",
    "    G = nx.MultiGraph()\n",
    "#     for row in flat_list:\n",
    "#         #print row\n",
    "    for first,second in flat_list:\n",
    "        G.add_node(first)\n",
    "        G.add_node(second)\n",
    "        #print first,second\n",
    "        G.add_edge(first,second)\n",
    "    \n",
    "    return sorted(G.degree().items(), key = lambda x: -x[1])[:100]\n",
    "    #print nx.pagerank_scipy(G)\n",
    "\n",
    "result = degree()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jean Shafiroff', 694),\n",
       " ('Gillian Miniter', 512),\n",
       " ('Mark Gilbertson', 469),\n",
       " ('Geoffrey Bradfield', 365),\n",
       " ('Somers Farkas', 313),\n",
       " ('Eleanora Kennedy', 295),\n",
       " ('Sharon Bush', 293),\n",
       " ('Debbie Bancroft', 288),\n",
       " ('Kamie Lightburn', 277),\n",
       " ('Andrew Saffir', 273),\n",
       " ('Alina Cho', 272),\n",
       " ('Michael Bloomberg', 265),\n",
       " ('Bonnie Comley', 263),\n",
       " ('Jamee Gregory', 253),\n",
       " ('Muffie Potter Aston', 247),\n",
       " ('Alexandra Lebenthal', 233),\n",
       " ('Allison Aston', 231),\n",
       " ('Lucia Hwong Gordon', 226),\n",
       " ('Mario Buatta', 223),\n",
       " ('nald', 221),\n",
       " ('Stewart Lane', 213),\n",
       " ('rmott', 203),\n",
       " ('Bettina Zilkha', 202),\n",
       " ('Barbara Tober', 197),\n",
       " ('Patrick McMullan', 191),\n",
       " ('Yaz Hernandez', 188),\n",
       " ('Sylvester Miniter', 187),\n",
       " ('Deborah Norville', 186),\n",
       " ('Ellen V. Futter', 185),\n",
       " ('Karen LeFrak', 181),\n",
       " ('Lydia Fenet', 181),\n",
       " ('Roric Tobin', 180),\n",
       " ('Liz Peek', 177),\n",
       " ('Daniel Benedict', 176),\n",
       " ('Audrey Gruss', 175),\n",
       " ('Liliana Cavendish', 173),\n",
       " ('Grace Meigher', 172),\n",
       " ('Amy Fine Collins', 172),\n",
       " ('Museum', 171),\n",
       " ('Diana Taylor', 169),\n",
       " ('Michele Herbert', 169),\n",
       " ('Barbara Regna', 167),\n",
       " ('Elizabeth Stribling', 166),\n",
       " ('Amy Hoadley', 165),\n",
       " ('Martha Stewart', 165),\n",
       " ('Dennis Basso', 165),\n",
       " ('Nicole Miller', 164),\n",
       " ('Evelyn Lauder', 163),\n",
       " ('Margo Langenberg', 163),\n",
       " ('Jennifer Creel', 160),\n",
       " ('Paula Zahn', 158),\n",
       " ('Margo Catsimatidis', 155),\n",
       " ('Wilbur Ross', 155),\n",
       " ('John Catsimatidis', 153),\n",
       " ('Donna Karan', 152),\n",
       " ('Rosanna Scotto', 150),\n",
       " ('Fe Fendi', 149),\n",
       " ('Felicia Taylor', 149),\n",
       " ('Karen Klopp', 145),\n",
       " ('Diana DiMenna', 144),\n",
       " ('Nathalie Kaplan', 142),\n",
       " ('Russell Simmons', 142),\n",
       " ('John Demsey', 141),\n",
       " ('Chris Meigher', 141),\n",
       " ('Ashley', 140),\n",
       " ('Coco Kopelman', 139),\n",
       " ('Tory Burch', 139),\n",
       " ('Allison Mignone', 139),\n",
       " ('Janna Bullock', 139),\n",
       " ('Patricia Shiah', 138),\n",
       " ('Richard Johnson', 137),\n",
       " ('Cynthia Lufkin', 137),\n",
       " ('CeCe Black', 136),\n",
       " ('Gregory Long', 135),\n",
       " ('Adelina Wong Ettelson', 135),\n",
       " ('Wendy Carduner', 135),\n",
       " ('Christopher Hyland', 134),\n",
       " ('Martha Glass', 134),\n",
       " ('Anne Hearst McInerney', 133),\n",
       " ('Anka Palitz', 133),\n",
       " ('Leonard Lauder', 133),\n",
       " ('Lizzie Tisch', 132),\n",
       " ('Alexia Hamm Ryan', 132),\n",
       " ('Kipton Cronkite', 131),\n",
       " ('R. Couri Hay', 131),\n",
       " ('Deborah Roberts', 130),\n",
       " ('Margaret Russell', 130),\n",
       " ('Gerald Loughlin', 130),\n",
       " ('Susan Magazine', 129),\n",
       " ('Guest', 129),\n",
       " ('Hilary Geary Ross', 127),\n",
       " ('Annette Rickel', 127),\n",
       " ('David Koch', 127),\n",
       " ('Susan Shin', 127),\n",
       " ('Amy McFarland', 126),\n",
       " ('Julia Koch', 125),\n",
       " ('Dawne Marie Grannum', 124),\n",
       " ('Frederick Anderson', 123),\n",
       " ('Alec Baldwin', 123),\n",
       " ('Hunt Slonem', 122)]"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.92\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#import heapq  # Heaps are efficient structures for tracking the largest\n",
    "              # elements in a collection.  Use introspection to find the\n",
    "              # function you need.\n",
    "#def degree():\n",
    " #   print [('Alec Baldwin', 144)] * 10\n",
    "\n",
    "\n",
    "grader.score(question_name='graph__degree', func=degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 4: pagerank\n",
    "\n",
    "A similar way to determine popularity is to look at their\n",
    "[pagerank](http://en.wikipedia.org/wiki/PageRank).  Pagerank is used for web ranking and was originally\n",
    "[patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by Google and is essentially the stationary distribution of a [markov\n",
    "chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph.\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 0.0001841088\n",
    "    \"std\": 0.0000758068\n",
    "    \"min\": 0.0001238355\n",
    "    \"25%\": 0.0001415028\n",
    "    \"50%\": 0.0001616183\n",
    "    \"75%\": 0.0001972663\n",
    "    \"max\": 0.0006085816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pagerank():\n",
    "    G = nx.MultiGraph()\n",
    "    for row in all_relationship:\n",
    "        #print row\n",
    "        for first,second in row:\n",
    "            G.add_node(first)\n",
    "            G.add_node(second)\n",
    "            G.add_edge(first,second)\n",
    "    \n",
    "    #return sorted(G.degree().items(), key = lambda x: -x[1])[:100]\n",
    "    return sorted(nx.pagerank_scipy(G).items(), key = lambda x: -x[1])[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.93\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#def pagerank():\n",
    "#    return [('Martha Stewart', 0.00019312108706213307)] * 100\n",
    "\n",
    "grader.score(question_name='graph__pagerank', func=pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 5: best_friends\n",
    "\n",
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights.\n",
    "\n",
    "Google these people and see what their connection is.  Can we use this to detect instances of infidelity?\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 25.84\n",
    "    \"std\": 16.0395470855\n",
    "    \"min\": 14.0\n",
    "    \"25%\": 16.0\n",
    "    \"50%\": 19.0\n",
    "    \"75%\": 29.25\n",
    "    \"max\": 109.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207505"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Gillian Miniter', 'Sylvester Miniter'), 60),\n",
       " (('Stewart Lane', 'Bonnie Comley'), 59),\n",
       " (('Sylvester Miniter', 'Gillian Miniter'), 57),\n",
       " (('Jamee Gregory', 'Peter Gregory'), 50),\n",
       " (('Ashley', 'rmott'), 47),\n",
       " (('Andrew Saffir', 'Daniel Benedict'), 46),\n",
       " (('Jean Shafiroff', 'Martin Shafiroff'), 42),\n",
       " (('Geoffrey Bradfield', 'Roric Tobin'), 35),\n",
       " (('Barbara Tober', 'Donald Tober'), 34),\n",
       " (('Jonathan Farkas', 'Somers Farkas'), 32)]"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_friends = Counter(flat_list).most_common()[:100]\n",
    "#best_friends = sorted(Counter(flat_list))#, key = lambda x: -x[1])[:100]\n",
    "best_friends[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = nx.MultiGraph()\n",
    "# for row in all_relationship:\n",
    "#     #print row\n",
    "for first,second in flat_list:\n",
    "    G.add_node(first)\n",
    "    G.add_node(second)\n",
    "    G.add_edge(first,second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def best_friends():\n",
    "    dict_friends = {}\n",
    "    for g in G.edges():\n",
    "        if g in dict_friends:\n",
    "            dict_friends[g] += 1\n",
    "        elif g[::-1] in dict_friends:\n",
    "            dict_friends[g] += 1\n",
    "        else:\n",
    "            dict_friends[g] = 1\n",
    "    \n",
    "    return sorted(dict_friends.items(),key=operator.itemgetter(1),reverse=True)[:100]\n",
    "\n",
    "#best_friends()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Gillian Miniter', 'Sylvester Miniter'), 117),\n",
       " (('Bonnie Comley', 'Stewart Lane'), 82)]"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_friends_flatlist():\n",
    "    dict_friends = {}\n",
    "    for g in flat_list:\n",
    "        if g in dict_friends:\n",
    "            dict_friends[g] += 1\n",
    "        elif g in dict_friends:\n",
    "            dict_friends[(g)] += 1\n",
    "        else:\n",
    "            dict_friends[g] = 1\n",
    "    \n",
    "    return sorted(dict_friends.items(),key=operator.itemgetter(1),reverse=True)[:100]\n",
    "\n",
    "best_friends()[:2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.9\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#def best_friends():\n",
    "#    return [(('Michael Kennedy', 'Eleanora Kennedy'), 41)] * 100\n",
    "\n",
    "grader.score(question_name='graph__best_friends', func=best_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
